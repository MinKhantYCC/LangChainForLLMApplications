{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm modules\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "from transformers import GenerationConfig\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "\n",
    "# prompt modules\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.prompts.chat import HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
    "from langchain.agents import load_tools, initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain_experimental.utilities.python import PythonREPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"minkhantycc/Llama-2-7b-chat-finetune-quantized\"\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "# bnb config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "# base_model.config.pretraining_tp = 1\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "base_model.generation_config = GenerationConfig(\n",
    "    max_new_tokens = 256,\n",
    "    temperature = 0.01,\n",
    "    repetition_penalty = 1.15,\n",
    "    do_sample = False,\n",
    "    eos_token_id = tokenizer.eos_token_id,\n",
    "    pad_token_id = tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# pipeline\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=base_model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=device_map,\n",
    "    return_full_text=False\n",
    ")\n",
    "\n",
    "# llm\n",
    "# hf_pipe = CustomHuggingFacePipeline(pipe, prompt_template=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, PromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    input_variables=['input'],\n",
    "    partial_variables={\n",
    "        'agent_scratchpad': \"\",\n",
    "        'tools': (\n",
    "            'wikipedia - A wrapper around Wikipedia. Useful for general questions about people, places, companies, facts, historical events, or other subjects. '\n",
    "            'Input should be a search query.\\n'\n",
    "            'Calculator - Useful for math-related questions.'\n",
    "        ),\n",
    "        'tool_names': 'wikipedia, Calculator'\n",
    "    },\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate(\n",
    "            prompt=PromptTemplate(\n",
    "                input_variables=['tool_names', 'tools'], \n",
    "                template=(\n",
    "                    '<|system|>Answer the following questions as best you can. You have access to the following tools:</s>\\n\\n'\n",
    "                    '{tools}\\n\\n'\n",
    "                    'Use the tools by specifying a JSON blob with an `action` key (tool name) and an `action_input` key (input to the tool).</s>\\n\\n'\n",
    "                    'The \"action\" field should only contain: {tool_names}</s>.\\n\\n'\n",
    "                    'The $JSON_BLOB should only contain a SINGLE action. Do NOT return a list of actions. Here is an example:</s>\\n\\n'\n",
    "                    '```\\n{{\\n  \"action\": \"$TOOL_NAME\",\\n  \"action_input\": \"$INPUT\"\\n}}\\n```\\n\\n'\n",
    "                    'REMEMBER: ALWAYS follow this format:</s>\\n\\n'\n",
    "                    'Question: the input question you must answer</s>\\n'\n",
    "                    'Thought: consider what action to take</s>\\n'\n",
    "                    'Action:</s>\\n'\n",
    "                    '```\\n$JSON_BLOB\\n```\\n'\n",
    "                    'Observation: the result of the action</s>\\n'\n",
    "                    '... (repeat Thought/Action/Observation as needed)</s>\\n'\n",
    "                    'Thought: I now know the final answer</s>\\n'\n",
    "                    'Final Answer: the final answer to the original input question</s>\\n\\n'\n",
    "                    'Begin! '\n",
    "                    'REMEMBER: Always use the exact phrase `Final Answer` in your response.</s>'\n",
    "                    '<|user|>'\n",
    "                )\n",
    "            )\n",
    "        ), \n",
    "        HumanMessagePromptTemplate(\n",
    "            prompt=PromptTemplate(\n",
    "                input_variables=['input', 'agent_scratchpad'],\n",
    "                template='{input}\\n\\n{agent_scratchpad}</s><|assistant|>'\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "class CustomHuggingFacePipeline(HuggingFacePipeline):\n",
    "    prompt_template: ChatPromptTemplate = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        super().__post_init__()\n",
    "\n",
    "    def _generate(self, inputs, **kwargs):\n",
    "        if self.prompt_template:\n",
    "            # Reformat the input text using the prompt template\n",
    "            inputs = self.prompt_template.format(input=inputs)\n",
    "            with open(\"log.txt\", \"+a\") as f:\n",
    "                f.write(inputs)\n",
    "        return super()._generate(prompts=[inputs], **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm\n",
    "hf_pipe = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Testing 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import Tool, load_tool, ReactCodeAgent\n",
    "# from transformers.agents.llm_engine import HfEngine\n",
    "# from langchain.agents import load_tools\n",
    "\n",
    "# # create engine\n",
    "# # engine = TransformersEngine(hf_pipe)\n",
    "# engine = HfEngine(model=\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "\n",
    "# # agent\n",
    "# langchain_tools = load_tools(['llm-math', 'wikipedia'], llm=hf_pipe)\n",
    "# math_tool = Tool.from_langchain(langchain_tools[0])\n",
    "# wiki_tool = Tool.from_langchain(langchain_tools[1])\n",
    "\n",
    "# # agent\n",
    "# agent = ReactCodeAgent(tools=[math_tool, wiki_tool], llm_engine=engine)\n",
    "\n",
    "# agent.run(\"What is 25% of 300?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = \"\"\"<|system|>Answer the following questions as best you can. You have access to the following tools:\"\"\"\n",
    "FORMAT_INSTRUCTIONS = \"\"\"The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```\" and \"```\":\\n\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\"\"\"\n",
    "SUFFIX = \"\"\"Begin!</s>\n",
    "\n",
    "<|user|>\n",
    "Question: {input} </s>\n",
    "Thought:{agent_scratchpad}</s>\n",
    "<|assistant|>\"\"\"\n",
    "\n",
    "## math and wikipedia tools\n",
    "tools = load_tools([\"llm-math\", \"wikipedia\"], llm=hf_pipe)\n",
    "\n",
    "# initialize agent\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=hf_pipe,\n",
    "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    "    # agent_kwargs={\n",
    "    #     'prefix': PREFIX,\n",
    "    #     'format_instructions': FORMAT_INSTRUCTIONS,\n",
    "    #     'suffix': SUFFIX\n",
    "    # }\n",
    ")\n",
    "\n",
    "agent.agent.llm_chain.prompt = prompt\n",
    "\n",
    "import langchain\n",
    "langchain.debug=True\n",
    "# math agent test\n",
    "result = agent(\"What is the 25% of 300?\")\n",
    "langchain.debug=False\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikipedia agent test\n",
    "question = \"Tom M. Mitchell is an American computer scientist \\\n",
    "and the Founders University Professor at Carnegie Mellon University (CMU)\\\n",
    "what book did he write?\"\n",
    "result = agent(question)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python agent\n",
    "agent = create_python_agent(\n",
    "    hf_pipe.bind(stop=[\"Human:\"]),\n",
    "    tool=PythonREPLTool(),\n",
    "    verbose=True,\n",
    "    # agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    agent_executor_kwargs={\"handle_parsing_errors\":True},\n",
    "    # agent_kwargs={\n",
    "    #     'prefix': PREFIX,\n",
    "    #     'format_instructions': FORMAT_INSTRUCTIONS,\n",
    "    #     'suffix': SUFFIX\n",
    "    # }\n",
    ")\n",
    "\n",
    "agent.agent.llm_chain.prompt = prompt\n",
    "\n",
    "# input to python\n",
    "customer_list = [\n",
    "    [\"Harrison\", \"Chase\"],\n",
    "    [\"Lang\", \"Chain\"],\n",
    "    [\"Dolly\", \"Too\"],\n",
    "    [\"Elle\", \"Elem\"],\n",
    "    [\"Geoff\",\"Fusion\"],\n",
    "    [\"Trance\",\"Former\"],\n",
    "    [\"Jen\",\"Ayai\"]\n",
    "]\n",
    "langchain.debug=True\n",
    "agent.run(\n",
    "    f\"\"\"Sort these custormers by last name and then first name \\\n",
    "    and print the output: {customer_list}\"\"\"\n",
    ")\n",
    "langchain.debug=False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
