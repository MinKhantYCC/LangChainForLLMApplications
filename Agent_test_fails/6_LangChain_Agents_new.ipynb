{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0d11cccb2fe14863948518a50205530f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b3898fe9f87b459d82cf7e45477351f6",
              "IPY_MODEL_4dc13232722b482e8e460184d171fc66",
              "IPY_MODEL_c37aa4f99c1941aea296da39e522f9d2"
            ],
            "layout": "IPY_MODEL_2b442046649447e49139437089b24225"
          }
        },
        "b3898fe9f87b459d82cf7e45477351f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9655bea5d8d84c3a8fff5f36812758ec",
            "placeholder": "​",
            "style": "IPY_MODEL_eca4f25ec7844f10a0c4b8287f136d32",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "4dc13232722b482e8e460184d171fc66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d116476e8f624cccb5405f5876920e72",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ef018946ca624fb1b303c40d9b3fa898",
            "value": 2
          }
        },
        "c37aa4f99c1941aea296da39e522f9d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3913a7e9ef3a4beb9d5169768c2ea6c9",
            "placeholder": "​",
            "style": "IPY_MODEL_3506499f908748f6a272ce0c67f02bd5",
            "value": " 2/2 [00:59&lt;00:00, 27.06s/it]"
          }
        },
        "2b442046649447e49139437089b24225": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9655bea5d8d84c3a8fff5f36812758ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eca4f25ec7844f10a0c4b8287f136d32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d116476e8f624cccb5405f5876920e72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef018946ca624fb1b303c40d9b3fa898": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3913a7e9ef3a4beb9d5169768c2ea6c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3506499f908748f6a272ce0c67f02bd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Environment"
      ],
      "metadata": {
        "id": "mRai3HzGIyFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%capture\n",
        "# %pip install langchain\n",
        "# %pip install langchain-hub\n",
        "# %pip install langchain-community langchain-huggingface\n",
        "# %pip install huggingface_hub transformers\n",
        "# %pip install sentence_transformers==2.2.2\n",
        "# %pip install chromadb faiss accelerate\n",
        "# %pip install -U bitsandbytes\n",
        "# %pip install tiktoken python-dotenv\n",
        "# %pip install faiss-gpu\n",
        "# %pip install InstructorEmbedding docarray\n",
        "# %pip install langchain_experimental\n",
        "# %pip install wikipedia\n",
        "# %pip install numexpr"
      ],
      "metadata": {
        "id": "N9ckcP72fWN_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import modules"
      ],
      "metadata": {
        "id": "IfnA-R2wI12_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3W0IlIlvfRb3"
      },
      "outputs": [],
      "source": [
        "# llm modules\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    pipeline\n",
        ")\n",
        "from transformers import GenerationConfig\n",
        "from langchain_huggingface.llms import HuggingFacePipeline\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "\n",
        "# prompt modules\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.prompts.chat import ChatPromptTemplate\n",
        "from langchain.prompts.chat import HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
        "from langchain_core.output_parsers.string import StrOutputParser\n",
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain.schema import SystemMessage\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Llama Quantized Model"
      ],
      "metadata": {
        "id": "Jl7hW8VsI4GM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"minkhantycc/Llama-2-7b-chat-finetune-quantized\"\n",
        "# model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "device_map = {\"\": 0}\n",
        "\n",
        "# bnb config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=\"float16\",\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "# base model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map\n",
        ")\n",
        "base_model.config.use_cache = False\n",
        "base_model.config.pretraining_tp = 1\n",
        "\n",
        "# Reload tokenizer to save it\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "base_model.generation_config = GenerationConfig(\n",
        "    max_new_tokens = 256,\n",
        "    temperature = 0.000001,\n",
        "    repetition_penalty = 1.15,\n",
        "    do_sample = False,\n",
        "    eos_token_id = tokenizer.eos_token_id,\n",
        "    pad_token_id = tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "# pipeline\n",
        "pipe = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=base_model,\n",
        "    tokenizer=tokenizer,\n",
        "    device_map=device_map,\n",
        "    return_full_text=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "0d11cccb2fe14863948518a50205530f",
            "b3898fe9f87b459d82cf7e45477351f6",
            "4dc13232722b482e8e460184d171fc66",
            "c37aa4f99c1941aea296da39e522f9d2",
            "2b442046649447e49139437089b24225",
            "9655bea5d8d84c3a8fff5f36812758ec",
            "eca4f25ec7844f10a0c4b8287f136d32",
            "d116476e8f624cccb5405f5876920e72",
            "ef018946ca624fb1b303c40d9b3fa898",
            "3913a7e9ef3a4beb9d5169768c2ea6c9",
            "3506499f908748f6a272ce0c67f02bd5"
          ]
        },
        "id": "lkSo22AWjNmd",
        "outputId": "852d6b62-2ba1-4944-c067-432e0fecdcd4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0d11cccb2fe14863948518a50205530f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create A Prompt Template"
      ],
      "metadata": {
        "id": "NwgmLdaNI-Pu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, PromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate(\n",
        "    input_variables=['input'],\n",
        "    partial_variables={\n",
        "        'agent_scratchpad': \"\",\n",
        "        'tools': (\n",
        "            'wikipedia - A wrapper around Wikipedia. Useful for general questions about people, places, companies, facts, historical events, or other subjects. '\n",
        "            'Input should be a search query.\\n'\n",
        "            'Calculator - Useful for math-related questions.'\n",
        "        ),\n",
        "        'tool_names': 'wikipedia, Calculator'\n",
        "    },\n",
        "    messages=[\n",
        "        SystemMessagePromptTemplate(\n",
        "            prompt=PromptTemplate(\n",
        "                input_variables=['tool_names', 'tools'],\n",
        "                template=(\n",
        "                    '<s>[INST]Answer the following questions as best you can. You have access to the following tools:\\n\\n'\n",
        "                    '{tools}\\n\\n'\n",
        "                    'Use the tools by specifying a JSON blob with an `action` key (tool name) and an `action_input` key (input to the tool).\\n\\n'\n",
        "                    'The \"action\" field should only contain: {tool_names}.\\n\\n'\n",
        "                    'The $JSON_BLOB should only contain a SINGLE action. Do NOT return a list of actions. Here is an example:\\n\\n'\n",
        "                    '```\\n{{\\n  \"action\": \"$TOOL_NAME\",\\n  \"action_input\": \"$INPUT\"\\n}}\\n```\\n\\n'\n",
        "                    'ALWAYS follow this format:\\n\\n'\n",
        "                    'Question: the input question you must answer\\n'\n",
        "                    'Thought: consider what action to take\\n'\n",
        "                    'Action:\\n'\n",
        "                    '```\\n$JSON_BLOB\\n```\\n'\n",
        "                    'Observation: the result of the action\\n'\n",
        "                    '... (repeat Thought/Action/Observation as needed)\\n'\n",
        "                    'Thought: I now know the final answer\\n'\n",
        "                    'Final Answer: the final answer to the original input question\\n\\n'\n",
        "                    'Begin! Always use the exact phrase `Final Answer` in your response.'\n",
        "                )\n",
        "            )\n",
        "        ),\n",
        "        HumanMessagePromptTemplate(\n",
        "            prompt=PromptTemplate(\n",
        "                input_variables=['input', 'agent_scratchpad'],\n",
        "                template='{input}\\n\\n{agent_scratchpad}[/INST]'\n",
        "            )\n",
        "        )\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "_eM6HmBqjFd3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create A Custom Pipeline"
      ],
      "metadata": {
        "id": "dIL8_YCfJFml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel\n",
        "class CustomHuggingFacePipeline(HuggingFacePipeline):\n",
        "    prompt_template: ChatPromptTemplate = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        super().__post_init__()\n",
        "\n",
        "    def _generate(self, inputs, **kwargs):\n",
        "        if self.prompt_template:\n",
        "            # Reformat the input text using the prompt template\n",
        "            inputs = self.prompt_template.format(input=inputs)\n",
        "        return super()._generate(prompts=[inputs], **kwargs)"
      ],
      "metadata": {
        "id": "cfZjyJa0xyCO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# llm\n",
        "hf_pipe = CustomHuggingFacePipeline(pipeline=pipe, prompt_template=prompt)"
      ],
      "metadata": {
        "id": "0zmKb8SRx3pE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Agent"
      ],
      "metadata": {
        "id": "bPGqpS2lJKMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
        "from langchain.agents import load_tools, initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
        "from langchain_experimental.utilities.python import PythonREPL"
      ],
      "metadata": {
        "id": "e0lLdSXujRJZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## math and wikipedia tools\n",
        "tools = load_tools([\"llm-math\", \"wikipedia\"], llm=hf_pipe)\n",
        "\n",
        "# initialize agent\n",
        "agent = initialize_agent(\n",
        "    tools,\n",
        "    hf_pipe,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True,\n",
        "    handle_parsing_errors=True\n",
        ")\n",
        "\n",
        "import langchain\n",
        "langchain.debug=True\n",
        "# math agent test\n",
        "result = agent(\"What is the 25% of 300?\")\n",
        "langchain.debug=False\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEupRH4VjS6p",
        "outputId": "b5bfc0d9-1c85-41c5-ab1c-f1b74e1b4474"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"What is the 25% of 300?\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"What is the 25% of 300?\",\n",
            "  \"agent_scratchpad\": \"\",\n",
            "  \"stop\": [\n",
            "    \"\\nObservation:\",\n",
            "    \"\\n\\tObservation:\"\n",
            "  ]\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:CustomHuggingFacePipeline] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"Answer the following questions as best you can. You have access to the following tools:\\n\\nCalculator(*args: Any, callbacks: Union[List[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, **kwargs: Any) -> Any - Useful for when you need to answer questions about math.\\nwikipedia - A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [Calculator, wikipedia]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: What is the 25% of 300?\\nThought:\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:CustomHuggingFacePipeline] [3.73s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \" The 25% of 300 would be 75. \",\n",
            "        \"generation_info\": null,\n",
            "        \"type\": \"Generation\"\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": null,\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] [3.73s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"text\": \" The 25% of 300 would be 75. \"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[chain:AgentExecutor > tool:_Exception] Entering Tool run with input:\n",
            "\u001b[0m\"Invalid Format: Missing 'Action:' after 'Thought:\"\n",
            "\u001b[36;1m\u001b[1;3m[tool/end]\u001b[0m \u001b[1m[chain:AgentExecutor > tool:_Exception] [1ms] Exiting Tool run with output:\n",
            "\u001b[0m\"Invalid Format: Missing 'Action:' after 'Thought:\"\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"What is the 25% of 300?\",\n",
            "  \"agent_scratchpad\": \" The 25% of 300 would be 75. \\nObservation: Invalid Format: Missing 'Action:' after 'Thought:\\nThought:\",\n",
            "  \"stop\": [\n",
            "    \"\\nObservation:\",\n",
            "    \"\\n\\tObservation:\"\n",
            "  ]\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:CustomHuggingFacePipeline] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"Answer the following questions as best you can. You have access to the following tools:\\n\\nCalculator(*args: Any, callbacks: Union[List[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, **kwargs: Any) -> Any - Useful for when you need to answer questions about math.\\nwikipedia - A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [Calculator, wikipedia]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: What is the 25% of 300?\\nThought: The 25% of 300 would be 75. \\nObservation: Invalid Format: Missing 'Action:' after 'Thought:\\nThought:\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:CustomHuggingFacePipeline] [4.62s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \" Final Answer: 75\\n\\nYou are asking me to calculate 25% of 300. This means that if we divide 300 by 4 (which is equal to 100), then we will get 75. So, the answer is 75.\",\n",
            "        \"generation_info\": null,\n",
            "        \"type\": \"Generation\"\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": null,\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] [4.62s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"text\": \" Final Answer: 75\\n\\nYou are asking me to calculate 25% of 300. This means that if we divide 300 by 4 (which is equal to 100), then we will get 75. So, the answer is 75.\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor] [8.37s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"75\\n\\nYou are asking me to calculate 25% of 300. This means that if we divide 300 by 4 (which is equal to 100), then we will get 75. So, the answer is 75.\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wikipedia agent test\n",
        "question = \"Tom M. Mitchell is an American computer scientist \\\n",
        "and the Founders University Professor at Carnegie Mellon University (CMU)\\\n",
        "what book did he write?\"\n",
        "result = agent(question)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-4umsK-GAzq",
        "outputId": "6e73ea3e-056f-4892-c59a-f2e2031598e6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m Final Answer: The book written by Tom M. Mitchell is \"Artificial Intelligence: A Modern Approach\".\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Tom M. Mitchell is an American computer scientist and the Founders University Professor at Carnegie Mellon University (CMU)what book did he write?',\n",
              " 'output': 'The book written by Tom M. Mitchell is \"Artificial Intelligence: A Modern Approach\".'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# python agent\n",
        "agent = create_python_agent(\n",
        "    hf_pipe,\n",
        "    tool=PythonREPLTool(),\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# input to python\n",
        "customer_list = [\n",
        "    [\"Harrison\", \"Chase\"],\n",
        "    [\"Lang\", \"Chain\"],\n",
        "    [\"Dolly\", \"Too\"],\n",
        "    [\"Elle\", \"Elem\"],\n",
        "    [\"Geoff\",\"Fusion\"],\n",
        "    [\"Trance\",\"Former\"],\n",
        "    [\"Jen\",\"Ayai\"]\n",
        "]\n",
        "\n",
        "agent.run(\n",
        "    f\"\"\"Sort these custormers by last name and then first name \\\n",
        "    and print the output: {customer_list}\"\"\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "oJ_cfRF6IZ5e",
        "outputId": "941b688d-153f-4fb2-ce2d-c9d1686f7e2e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m Final Answer: The customers are sorted alphabetically according to their last names followed by their first names.\n",
            "\n",
            "Customers:\n",
            "\n",
            "1. Harrison Chase\n",
            "2. Lang Chain\n",
            "3. Dolly Too\n",
            "4. Elle Elem\n",
            "5. Geoff Fusion\n",
            "6. Trance Former\n",
            "7. Jen Ayai\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The customers are sorted alphabetically according to their last names followed by their first names.\\n\\nCustomers:\\n\\n1. Harrison Chase\\n2. Lang Chain\\n3. Dolly Too\\n4. Elle Elem\\n5. Geoff Fusion\\n6. Trance Former\\n7. Jen Ayai'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}