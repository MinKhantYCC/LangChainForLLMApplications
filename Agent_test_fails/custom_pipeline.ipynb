{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm modules\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "from transformers import GenerationConfig\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "\n",
    "# chain\n",
    "from langchain.chains.conversation.base import ConversationChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "\n",
    "# prompt modules\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.prompts.chat import HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "# base modules\n",
    "import os\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"<|system|>Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "wikipedia: A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.\n",
    "Calculator: Useful for when you need to answer questions about math.\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [wikipedia, Calculator]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}</s>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_pipe(HuggingFacePipeline):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __call__(self, prompt: str, **kwargs):\n",
    "        # system_message = prompt.split(\"Human:\")[0]\n",
    "        # user_message = prompt.split(\"Human:\")[1]\n",
    "        # formatted_prompt = f\"<|system|>{system_message}</s><|user|>{user_message}</s><|assistant|>\"\n",
    "        formatted_prompt = f\"<|system|>{prompt}</s><|assistant|>\"\n",
    "        print(formatted_prompt)\n",
    "        output = super()._generate(prompts=[formatted_prompt], **kwargs)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"minkhantycc/Llama-2-7b-chat-finetune-quantized\"\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "# bnb config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "base_model.generation_config = GenerationConfig(\n",
    "    max_new_tokens = 256,\n",
    "    temperature = 0.000001,\n",
    "    repetition_penalty = 1.15,\n",
    "    do_sample = False,\n",
    "    eos_token_id = tokenizer.eos_token_id,\n",
    "    pad_token_id = tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# pipeline\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=base_model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=device_map,\n",
    "    return_full_text=False\n",
    ")\n",
    "\n",
    "# llm\n",
    "hf_pipe = custom_pipe(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
    "from langchain.agents import load_tools, initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain_experimental.utilities.python import PythonREPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## math and wikipedia tools\n",
    "tools = load_tools([\"llm-math\", \"wikipedia\"], llm=hf_pipe)\n",
    "\n",
    "# initialize agent\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    hf_pipe,\n",
    "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.debug=True\n",
    "# math agent test\n",
    "agent(\"What is the 25% of 300?\")\n",
    "langchain.debug=False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
